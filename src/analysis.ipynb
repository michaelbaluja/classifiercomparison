{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Rid of pesky sk-learn version warnings since we aren't using those variables anyway\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filename, verbose=False):\n",
    "    '''\n",
    "    Loads dictionary of metrics from given filename\n",
    "    \n",
    "    Args:\n",
    "    - filename (str): file to load\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .pickle extension appended to filename if not already present\n",
    "    Return\n",
    "    - dictionary (dict): data found in file\n",
    "    - None (None): return None val in case exception is raised and dictionary file does not exist\n",
    "    '''\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'rb') as pickle_file: \n",
    "            dictionary = pickle.load(pickle_file) \n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loading variables\n",
    "data_dict = {}\n",
    "datasets = ['yelp', 'sub_ob', 'clickbait']\n",
    "models = ['svm', 'logreg', 'randomforest', 'nn']\n",
    "index = 2 # Loads final (of 3) data checkpoint for data\n",
    "\n",
    "# Load data\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        data_dict[(model, dataset)] = load_dict('../checkpoints/{model}/{model}_{dataset}_{index}'.format(model=model, dataset=dataset, index=index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_METRICS = ['acc_test', 'precision_test', 'recall_test', 'f1_test']\n",
    "TRAIN_METRICS = ['acc_train', 'precision_train', 'recall_train', 'f1_train']\n",
    "DATASETS = ['yelp', 'sub_ob', 'clickbait']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/train set performance (across 3 trials) for each algorithm/dataset combo (Raw + Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_values(values):\n",
    "    new_vals = []\n",
    "    for value in values:\n",
    "        if type(value) in (list, np.ndarray):\n",
    "            interior_list = []\n",
    "            for val in value:\n",
    "                interior_list.append(round(val, 3))\n",
    "            new_vals.append(interior_list)\n",
    "        else:\n",
    "            new_vals.append(round(value, 3))\n",
    "    return new_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_data_test_dict = {}\n",
    "algo_data_train_dict = {}\n",
    "\n",
    "for (algorithm, dataset), metric_dict in data_dict.items():\n",
    "    for metric in TEST_METRICS:\n",
    "        # Take the mean to get the precision, recall, and F1 (since not averaged initially)\n",
    "        values = [np.mean(metric_dict[(dataset, index)][metric]) for index in range(3)]\n",
    "        \n",
    "        ## Change metric name\n",
    "        #metric = metric[:metric.find('_')]\n",
    "        #if metric == 'acc': metric = 'ACC'\n",
    "        #elif metric == 'precision': metric = 'PREC'\n",
    "        #elif metric == 'recall': metric = 'REC'\n",
    "        #elif metric == 'f1': metric = 'F1'\n",
    "        \n",
    "        ## Change algorithm name\n",
    "        #if algorithm == 'svm': algorithm = 'SVM'\n",
    "        #elif algorithm == 'logreg': algorithm = 'LogReg'\n",
    "        #elif algorithm == 'randomforest': algorithm = 'RF'\n",
    "    \n",
    "        #algo_data_test_dict[(algorithm, dataset.capitalize(), metric)] = {'mean': np.mean(values), 'values': round_values(values)}\n",
    "        algo_data_test_dict[(algorithm, dataset, metric)] = {'mean': np.mean(values), 'values': values}\n",
    "    for metric in TRAIN_METRICS:\n",
    "        values = [metric_dict[(dataset, index)][metric] for index in range(3)]\n",
    "        algo_data_train_dict[(algorithm, dataset, metric)] = {'mean': np.mean(values), 'values': values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame.from_dict(algo_data_test_dict, orient='index')\n",
    "test_df.drop('mean', inplace=True, axis=1)\n",
    "test_df['values'] = test_df['values'].apply(list)\n",
    "test_df\n",
    "test_df.to_csv('../algodatatestdict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean test set performance (across 3 trials x 3 data sets) for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_algo_dict = {}\n",
    "\n",
    "for (algorithm, dataset, metric), value_dict in algo_data_test_dict.items():\n",
    "    try:\n",
    "        [mean_algo_dict[(algorithm, metric)].append(value) for value in value_dict['values']]\n",
    "    except KeyError:\n",
    "        mean_algo_dict[(algorithm, metric)] = value_dict['values']\n",
    "        \n",
    "#for key in mean_algo_dict.keys():\n",
    "#    mean_algo_dict[key] = np.mean(mean_algo_dict[key])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best algorithms\n",
    "datasets = ['yelp', 'sub_ob', 'clickbait']\n",
    "best_algo_1_dict, best_algo_2_dict, best_algo_3_dict = {}, {}, {}\n",
    "\n",
    "# Table 1\n",
    "best_algo_1_dict['acc_test'] = [np.mean(value) for value in algo_data_test_dict[('nn', 'clickbait', 'acc_test')]['values']]\n",
    "best_algo_1_dict['precision_test'] = [np.mean(value) for value in algo_data_test_dict[('nn', 'clickbait', 'precision_test')]['values']]\n",
    "best_algo_1_dict['recall_test'] = [np.mean(value) for value in algo_data_test_dict[('nn', 'sub_ob', 'recall_test')]['values']]\n",
    "best_algo_1_dict['f1_test'] = [np.mean(value) for value in algo_data_test_dict[('nn', 'clickbait', 'f1_test')]['values']]\n",
    "\n",
    "# Table 2\n",
    "best_algo_2_dict['acc_test'] = mean_algo_dict[('nn', 'acc_test')]\n",
    "best_algo_2_dict['precision_test'] = mean_algo_dict[('nn', 'precision_test')]\n",
    "best_algo_2_dict['recall_test'] = mean_algo_dict[('nn', 'recall_test')]\n",
    "best_algo_2_dict['f1_test'] = mean_algo_dict[('nn', 'f1_test')]\n",
    "\n",
    "# Table 3\n",
    "best_algo_3_dict['acc_train'] = [np.mean(value) for value in algo_data_train_dict[('nn', 'clickbait', 'acc_train')]['values']]\n",
    "best_algo_3_dict['precision_train'] = [np.mean(value) for value in algo_data_train_dict[('nn', 'clickbait', 'precision_train')]['values']]\n",
    "best_algo_3_dict['recall_train'] = [np.mean(value) for value in algo_data_train_dict[('nn', 'clickbait', 'recall_train')]['values']]\n",
    "best_algo_3_dict['f1_train'] = [np.mean(value) for value in algo_data_train_dict[('nn', 'clickbait', 'f1_train')]['values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: ('svm', 'yelp', 'acc_test')\n",
      "p: 1.1249249688436414e-05\n",
      "\n",
      "test: ('logreg', 'yelp', 'acc_test')\n",
      "p: 5.589064660055868e-06\n",
      "\n",
      "test: ('randomforest', 'yelp', 'acc_test')\n",
      "p: 6.240236280414811e-06\n",
      "\n",
      "test: ('nn', 'yelp', 'acc_test')\n",
      "p: 7.788995058657149e-05\n",
      "\n",
      "test: ('svm', 'sub_ob', 'acc_test')\n",
      "p: 4.353158535423539e-06\n",
      "\n",
      "test: ('logreg', 'sub_ob', 'acc_test')\n",
      "p: 3.947133673463829e-06\n",
      "\n",
      "test: ('randomforest', 'sub_ob', 'acc_test')\n",
      "p: 3.7585590837153907e-06\n",
      "\n",
      "test: ('nn', 'sub_ob', 'acc_test')\n",
      "p: 0.028727349432309892\n",
      "\n",
      "test: ('svm', 'clickbait', 'acc_test')\n",
      "p: 1.9139281877275435e-05\n",
      "\n",
      "test: ('logreg', 'clickbait', 'acc_test')\n",
      "p: 1.905593457949983e-05\n",
      "\n",
      "test: ('randomforest', 'clickbait', 'acc_test')\n",
      "p: 1.88527653641544e-05\n",
      "\n",
      "test: ('nn', 'clickbait', 'acc_test')\n",
      "p: 1.0\n",
      "\n",
      "test: ('svm', 'yelp', 'precision_test')\n",
      "p: 2.1542209453553922e-05\n",
      "\n",
      "test: ('logreg', 'yelp', 'precision_test')\n",
      "p: 1.2955782710460094e-05\n",
      "\n",
      "test: ('randomforest', 'yelp', 'precision_test')\n",
      "p: 1.3098570456567658e-05\n",
      "\n",
      "test: ('nn', 'yelp', 'precision_test')\n",
      "p: 0.03316118327644078\n",
      "\n",
      "test: ('svm', 'sub_ob', 'precision_test')\n",
      "p: 1.8544845589082943e-05\n",
      "\n",
      "test: ('logreg', 'sub_ob', 'precision_test')\n",
      "p: 1.2918946667757528e-05\n",
      "\n",
      "test: ('randomforest', 'sub_ob', 'precision_test')\n",
      "p: 8.757893065745393e-06\n",
      "\n",
      "test: ('nn', 'sub_ob', 'precision_test')\n",
      "p: 0.03462066002224069\n",
      "\n",
      "test: ('svm', 'clickbait', 'precision_test')\n",
      "p: 4.8736446485271215e-05\n",
      "\n",
      "test: ('logreg', 'clickbait', 'precision_test')\n",
      "p: 4.8708790102325005e-05\n",
      "\n",
      "test: ('randomforest', 'clickbait', 'precision_test')\n",
      "p: 4.8186009654388883e-05\n",
      "\n",
      "test: ('nn', 'clickbait', 'precision_test')\n",
      "p: 1.0\n",
      "\n",
      "test: ('svm', 'yelp', 'recall_test')\n",
      "p: 0.01311364035581817\n",
      "\n",
      "test: ('logreg', 'yelp', 'recall_test')\n",
      "p: 0.012711897132520116\n",
      "\n",
      "test: ('randomforest', 'yelp', 'recall_test')\n",
      "p: 0.01069962934201603\n",
      "\n",
      "test: ('nn', 'yelp', 'recall_test')\n",
      "p: 0.284983699487455\n",
      "\n",
      "test: ('svm', 'sub_ob', 'recall_test')\n",
      "p: 0.011155948777358366\n",
      "\n",
      "test: ('logreg', 'sub_ob', 'recall_test')\n",
      "p: 0.010681227849335747\n",
      "\n",
      "test: ('randomforest', 'sub_ob', 'recall_test')\n",
      "p: 0.00938548737162736\n",
      "\n",
      "test: ('nn', 'sub_ob', 'recall_test')\n",
      "p: 1.0\n",
      "\n",
      "test: ('svm', 'clickbait', 'recall_test')\n",
      "p: 0.03551267878622382\n",
      "\n",
      "test: ('logreg', 'clickbait', 'recall_test')\n",
      "p: 0.03572085684558541\n",
      "\n",
      "test: ('randomforest', 'clickbait', 'recall_test')\n",
      "p: 0.03530937408647399\n",
      "\n",
      "test: ('nn', 'clickbait', 'recall_test')\n",
      "p: 0.734792881391845\n",
      "\n",
      "test: ('svm', 'yelp', 'f1_test')\n",
      "p: 1.1959361535004586e-05\n",
      "\n",
      "test: ('logreg', 'yelp', 'f1_test')\n",
      "p: 6.764524422502433e-06\n",
      "\n",
      "test: ('randomforest', 'yelp', 'f1_test')\n",
      "p: 6.2213798302159106e-06\n",
      "\n",
      "test: ('nn', 'yelp', 'f1_test')\n",
      "p: 0.08282930923405293\n",
      "\n",
      "test: ('svm', 'sub_ob', 'f1_test')\n",
      "p: 2.1943308415303543e-06\n",
      "\n",
      "test: ('logreg', 'sub_ob', 'f1_test')\n",
      "p: 8.281009322742191e-06\n",
      "\n",
      "test: ('randomforest', 'sub_ob', 'f1_test')\n",
      "p: 3.8914140038133056e-06\n",
      "\n",
      "test: ('nn', 'sub_ob', 'f1_test')\n",
      "p: 0.005313732599501632\n",
      "\n",
      "test: ('svm', 'clickbait', 'f1_test')\n",
      "p: 1.775857561585871e-05\n",
      "\n",
      "test: ('logreg', 'clickbait', 'f1_test')\n",
      "p: 1.7608356387301182e-05\n",
      "\n",
      "test: ('randomforest', 'clickbait', 'f1_test')\n",
      "p: 1.739105319032823e-05\n",
      "\n",
      "test: ('nn', 'clickbait', 'f1_test')\n",
      "p: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 1\n",
    "for best_metric, best_values in best_algo_1_dict.items():\n",
    "    for (model, dataset, metric), value_dict in algo_data_test_dict.items():\n",
    "        if best_metric == metric:\n",
    "            stat, p = ttest_ind(value_dict['values'], best_values)\n",
    "            print('test: {}\\np: {}\\n'.format((model, dataset, metric), p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: ('svm', 'acc_test')\n",
      "p: 0.00029056880933519245\n",
      "\n",
      "test: ('logreg', 'acc_test')\n",
      "p: 0.00023076718830527577\n",
      "\n",
      "test: ('randomforest', 'acc_test')\n",
      "p: 0.0001223460955157153\n",
      "\n",
      "test: ('nn', 'acc_test')\n",
      "p: 1.0\n",
      "\n",
      "test: ('svm', 'precision_test')\n",
      "p: 0.011572656646295318\n",
      "\n",
      "test: ('logreg', 'precision_test')\n",
      "p: 0.005866730324708857\n",
      "\n",
      "test: ('randomforest', 'precision_test')\n",
      "p: 0.002534969880948449\n",
      "\n",
      "test: ('nn', 'precision_test')\n",
      "p: 1.0\n",
      "\n",
      "test: ('svm', 'recall_test')\n",
      "p: 1.484662789107798e-11\n",
      "\n",
      "test: ('logreg', 'recall_test')\n",
      "p: 1.3013649507016699e-11\n",
      "\n",
      "test: ('randomforest', 'recall_test')\n",
      "p: 6.732947333304875e-12\n",
      "\n",
      "test: ('nn', 'recall_test')\n",
      "p: 1.0\n",
      "\n",
      "test: ('svm', 'f1_test')\n",
      "p: 1.7874243923286493e-07\n",
      "\n",
      "test: ('logreg', 'f1_test')\n",
      "p: 3.261990196074667e-07\n",
      "\n",
      "test: ('randomforest', 'f1_test')\n",
      "p: 3.3901419716765365e-07\n",
      "\n",
      "test: ('nn', 'f1_test')\n",
      "p: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table 2\n",
    "for best_metric, best_values in best_algo_2_dict.items():\n",
    "    for (algorithm, metric), value_dict in mean_algo_dict.items():\n",
    "        if best_metric == metric:\n",
    "            stat, p = ttest_ind(value_dict, best_values)\n",
    "            print('test: {}\\np: {}\\n'.format((algorithm, metric), p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3\n",
    "for best_metric, best_values in best_algo_3_dict.items():\n",
    "    for (model, dataset, metric), value_dict in algo_data_test_dict.items():\n",
    "        if best_metric == metric:\n",
    "            stat, p = ttest_ind(value_dict['values'], best_values)\n",
    "            if 0.05 < p < 1:\n",
    "                print(model, dataset, metric)\n",
    "                #print('test: {}\\nstat: {}\\np: {}'.format((model, dataset, metric), stat, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap(errors, D_list, title):\n",
    "    plt.figure(figsize = (2,4))\n",
    "    ax = sns.heatmap(errors, annot=True, fmt='.3f', yticklabels=D_list, xticklabels=[])\n",
    "    ax.collections[0].colorbar.set_label('error')\n",
    "    ax.set(ylabel='max depth D')\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(*lists):\n",
    "    '''\n",
    "    Computes element-wise mean for each element of the passed in lists. \n",
    "    If lists are uneven, shrinks lists to shortest size\n",
    "    Args: \n",
    "    - *lists (list-like): lists to perform averaging over\n",
    "    Returns:\n",
    "    - list of element-wise means\n",
    "    '''\n",
    "    return [np.mean(values) for values in zip(*lists)]\n",
    "\n",
    "assert get_mean([1,2,3], [4,5,6]) == [2.5,3.5,4.5]\n",
    "assert get_mean([1,1,1], []) == []\n",
    "assert get_mean([0,0,0], [2,4,6]) == [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Inconsistent shape between the condition and the input (got (132, 1) and (132,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9d6dc53466a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msvm_param_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc_vals\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdraw_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_validation_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bbf9f2ab1e7e>\u001b[0m in \u001b[0;36mdraw_heatmap\u001b[0;34m(errors, D_list, title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.3f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max depth D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[1;32m    511\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                           yticklabels, mask)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;31m# Add the pcolormesh kwargs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_matrix_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_where\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Get good names for the rows and columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36mmasked_where\u001b[0;34m(condition, a, copy)\u001b[0m\n\u001b[1;32m   1940\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcshape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mashape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m         raise IndexError(\"Inconsistent shape between the condition and the input\"\n\u001b[0;32m-> 1942\u001b[0;31m                          \" (got %s and %s)\" % (cshape, ashape))\n\u001b[0m\u001b[1;32m   1943\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Inconsistent shape between the condition and the input (got (132, 1) and (132,))"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 144x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gets the list of validation performance for each paremeter combo across all data sets and trials for SVM\n",
    "svm_validation_means = get_mean(*[data_dict[('svm', dataset)][(dataset, i)]['cv_results']['mean_test_score'] \n",
    "                    for dataset in DATASETS for i in range(3)])\n",
    "\n",
    "c_vals = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "svm_param_grid = list(ParameterGrid({'kernel': ['linear'], 'C': c_vals}))\n",
    "draw_heatmap(svm_validation_means, svm_param_grid, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
