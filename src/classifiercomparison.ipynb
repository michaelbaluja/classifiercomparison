{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install statements for all utilized libraries (uncomment which are needed)\n",
    "#!pip3 install pandas # installs numpy with it \n",
    "#!pip3 install numpy\n",
    "#!pip3 install pickle\n",
    "#!pip3 install sklearn\n",
    "#!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Misc\n",
    "import pickle # saving/loading metrics\n",
    "\n",
    "# ML\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dictionary, filename, verbose=False):\n",
    "    '''\n",
    "    Saves dictionary object as a pickle file for reloading and easy viewing\n",
    "    \n",
    "    Args:\n",
    "    - dictionary (dict): data to be saved\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .json extension appended to filename if not already present\n",
    "    Return:\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    '''\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "        \n",
    "    with open(filename, \"wb\") as outfile:  \n",
    "        pickle.dump(dictionary, outfile)\n",
    "        outfile.close()\n",
    "    \n",
    "    return filename\n",
    "        \n",
    "def load_dict(filename, verbose=False):\n",
    "    '''\n",
    "    Loads dictionary of metrics from given filename\n",
    "    \n",
    "    Args:\n",
    "    - filename (str): file to load\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .pickle extension appended to filename if not already present\n",
    "    Return\n",
    "    - dictionary (dict): data found in file\n",
    "    - None (None): return None val in case exception is raised and dictionary file does not exist\n",
    "    '''\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'rb') as pickle_file: \n",
    "            dictionary = pickle.load(pickle_file) \n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict to store {name: dataset}\n",
    "dataset_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer that turns text samples into token vector \n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# Load yelp data sets\n",
    "yelp_test_df = pd.read_csv('../data/yelp_review_polarity_csv/test.csv', names=['label', 'data']) \n",
    "yelp_train_df = pd.read_csv('../data/yelp_review_polarity_csv/train.csv', names=['label', 'data']) \n",
    "\n",
    "# Since yelp data set is already split into test and train, recombine\n",
    "yelp_df = pd.concat([yelp_test_df, yelp_train_df])\n",
    "\n",
    "# Data set is too large to work with in memory since I don't have 2TiB of RAM just lying around, so we're cutting the data down\n",
    "yelp_df = yelp_df.sample(n=16000,replace=False,axis='index')\n",
    "\n",
    "# Change 1, 2 label to 0, 1 for uniformity with other data sets\n",
    "# Data set has 1 for negative and 2 for positive, so we switch 0 to negative and 1 to positive\n",
    "yelp_df['label'] = yelp_df['label'].apply(lambda label: 0 if label == 1 else 1)\n",
    "\n",
    "#Vectorize\n",
    "yelp_df['data'] = vectorizer.fit_transform(yelp_df['data']).toarray()\n",
    "\n",
    "# Transform df to np array for easier use & add info to dict\n",
    "yelp_data = yelp_df.values\n",
    "dataset_dict['yelp'] = yelp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity/Objectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "subjectivity_df = pd.read_csv('../data/subjectobject/subjectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "objectivity_df = pd.read_csv('../data/subjectobject/objectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "\n",
    "# Add labels (subjective is 0, objective is 0)\n",
    "subjectivity_df['label'] = 0\n",
    "objectivity_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "sub_ob_df = pd.concat([subjectivity_df, objectivity_df])\n",
    "sub_ob_df = sub_ob_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "#Vectorize\n",
    "sub_ob_df['data'] = vectorizer.fit_transform(sub_ob_df['data']).toarray()\n",
    "\n",
    "#Transform df to np array, and add to dict\n",
    "sub_ob_data = sub_ob_df.values\n",
    "dataset_dict['sub_ob'] = sub_ob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "clickbait_df = pd.read_csv('../data/clickbait/clickbait_data', sep='\\n', names=['data'])\n",
    "nonclickbait_df = pd.read_csv('../data/clickbait/non_clickbait_data', sep='\\n', names=['data'])\n",
    "\n",
    "# Add labels (subjective is 0, objective is 0)\n",
    "nonclickbait_df['label'] = 0\n",
    "clickbait_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "clickbait_df = pd.concat([clickbait_df, nonclickbait_df])\n",
    "clickbait_df = clickbait_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "#Vectorize\n",
    "clickbait_df['data'] = vectorizer.fit_transform(clickbait_df['data']).toarray()\n",
    "\n",
    "#Transform df to np array, and add to dict\n",
    "clickbait_data = clickbait_df.values\n",
    "dataset_dict['clickbait'] = clickbait_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metric dict\n",
    "svm_metric_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of parameters to search over for SVM\n",
    "c_vals = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "param_grid = [{'kernel': ['linear'], 'C': c_vals}, {'kernel': ['poly'], 'degree': [0,2,3], 'C': c_vals}, {'kernel': ['rbf'], 'gamma': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2], 'C': c_vals}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model & grid search object\n",
    "svc = SVC()\n",
    "clf_svc = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, n_jobs=3, verbose=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 143 candidates, totalling 715 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed: 44.9min\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed: 90.2min\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed: 90.4min\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed: 90.6min\n",
      "[Parallel(n_jobs=3)]: Done 307 tasks      | elapsed: 90.8min\n",
      "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed: 91.0min\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed: 91.2min\n",
      "[Parallel(n_jobs=3)]: Done 386 tasks      | elapsed: 91.4min\n",
      "[Parallel(n_jobs=3)]: Done 415 tasks      | elapsed: 91.7min\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed: 91.9min\n",
      "[Parallel(n_jobs=3)]: Done 475 tasks      | elapsed: 92.2min\n",
      "[Parallel(n_jobs=3)]: Done 506 tasks      | elapsed: 92.4min\n",
      "[Parallel(n_jobs=3)]: Done 539 tasks      | elapsed: 92.6min\n",
      "[Parallel(n_jobs=3)]: Done 572 tasks      | elapsed: 92.9min\n",
      "[Parallel(n_jobs=3)]: Done 607 tasks      | elapsed: 93.2min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed: 93.5min\n",
      "[Parallel(n_jobs=3)]: Done 679 tasks      | elapsed: 94.2min\n",
      "[Parallel(n_jobs=3)]: Done 715 out of 715 | elapsed: 752.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 143 candidates, totalling 715 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:   58.1s\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed: 35.1min\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed: 35.3min\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed: 35.5min\n",
      "[Parallel(n_jobs=3)]: Done 307 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed: 35.9min\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed: 36.1min\n",
      "[Parallel(n_jobs=3)]: Done 386 tasks      | elapsed: 36.4min\n",
      "[Parallel(n_jobs=3)]: Done 415 tasks      | elapsed: 36.6min\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed: 36.8min\n",
      "[Parallel(n_jobs=3)]: Done 475 tasks      | elapsed: 37.1min\n",
      "[Parallel(n_jobs=3)]: Done 506 tasks      | elapsed: 37.3min\n",
      "[Parallel(n_jobs=3)]: Done 539 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=3)]: Done 572 tasks      | elapsed: 37.6min\n",
      "[Parallel(n_jobs=3)]: Done 607 tasks      | elapsed: 37.7min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed: 37.9min\n",
      "[Parallel(n_jobs=3)]: Done 679 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=3)]: Done 715 out of 715 | elapsed: 39.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 143 candidates, totalling 715 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:   57.3s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed: 75.2min\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed: 136.5min\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed: 136.7min\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed: 136.9min\n",
      "[Parallel(n_jobs=3)]: Done 307 tasks      | elapsed: 137.1min\n",
      "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed: 137.3min\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed: 137.5min\n",
      "[Parallel(n_jobs=3)]: Done 386 tasks      | elapsed: 137.7min\n",
      "[Parallel(n_jobs=3)]: Done 415 tasks      | elapsed: 137.9min\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed: 138.2min\n",
      "[Parallel(n_jobs=3)]: Done 475 tasks      | elapsed: 138.4min\n",
      "[Parallel(n_jobs=3)]: Done 506 tasks      | elapsed: 138.6min\n",
      "[Parallel(n_jobs=3)]: Done 539 tasks      | elapsed: 138.9min\n",
      "[Parallel(n_jobs=3)]: Done 572 tasks      | elapsed: 139.1min\n",
      "[Parallel(n_jobs=3)]: Done 607 tasks      | elapsed: 139.4min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed: 139.7min\n",
      "[Parallel(n_jobs=3)]: Done 679 tasks      | elapsed: 140.2min\n",
      "[Parallel(n_jobs=3)]: Done 715 out of 715 | elapsed: 152.8min finished\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in dataset_dict.items():\n",
    "    # Get data\n",
    "    X, y = dataset[:, 1:], dataset[:, :1] #Treats first column as label\n",
    "    for i in range(3):\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "        \n",
    "        clf_svc.fit(X_train, y_train.ravel()) # Fit training data to model\n",
    "        y_test_pred = clf_svc.predict(X_test) # Predict test values using best parameters from classifier\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "        \n",
    "        svm_metric_dict[(name, i)] = {'acc': acc, 'precision': precision, 'recall': recall, 'f1': f1, 'model': clf_svc, 'cv_results': clf_svc.cv_results_} # Add metrics to dict for analysis\n",
    "        save_dict(svm_metric_dict, '../checkpoints/svm/svm_{}_{}.pickle'.format(name, i)) # Save checkpoint results in case of hardware failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "###### DEPRACATED FOR MULTITHREAD SKLEARN GRID SEARCH, KEPT IN CASE OF MEASURING OTHER METRICS\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cycle across each param combo\n",
    "performance_dict = {}\n",
    "for param_dict in tqdm(list(ParameterGrid(param_grid))):\n",
    "    performance = 0\n",
    "    C, degree, gamma, kernel = param_dict.values()\n",
    "    if ((kernel in ('linear', 'rbf') and degree > 0) or  # Don't want to run linear or rbf with polynomial degrees (degree will be ignored but we'll get duplicate trials)\n",
    "        (kernel == 'poly' and degree == 0) or # Don't want polynomial with degree 0\n",
    "        (kernel in ('linear', 'poly') and gamma > 0) or # Don't want linear or poly with gamma param\n",
    "        (kernel == 'rbf' and gamma == 0)): # Don't want rbf with 0 gamma\n",
    "        continue\n",
    "    # Do k fold validation\n",
    "    for train, validate in kf.split(X_letter_train):\n",
    "        X_letter_train_cross, X_letter_val_cross, y_letter_train_cross, y_letter_val_cross = X_letter_train[train], X_letter_train[validate], y_letter_train[train], y_letter_train[validate] # get data folds\n",
    "        svm_letter = SVC(C=C, degree=degree, kernel=kernel) # create the model #NOTE: not scaling because all data appears to follow the same scaling regardless\n",
    "        svm_letter.fit(X_letter_train_cross, y_letter_train_cross.ravel()) # fit the model\n",
    "        y_letter_val_cross_pred = svm_letter.predict(X_letter_val_cross) # predict validation data\n",
    "        performance += accuracy_score(y_letter_val_cross, y_letter_val_cross_pred) # keep track of performance\n",
    "    # Average the performance\n",
    "    performance /= 5\n",
    "    \n",
    "    # Add performance info to dict\n",
    "    performance_dict[(C, degree, kernel)] = performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
