{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.1 Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install statements for all utilized libraries (uncomment which are needed)\n",
    "#!pip3 install pandas # installs numpy with it \n",
    "#!pip3 install numpy\n",
    "#!pip3 install pickle\n",
    "#!pip3 install sklearn\n",
    "#!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Misc\n",
    "import pickle # saving/loading metrics\n",
    "import os # creating necessary directory structure\n",
    "\n",
    "# ML\n",
    "# Classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Helper functions\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dictionary, filename, verbose=False):\n",
    "    '''\n",
    "    Saves dictionary object a,s a pickle file for reloading and easy viewing\n",
    "    \n",
    "    Params:\n",
    "    - dictionary (dict): data to be saved\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .pickle \n",
    "        extension appended to filename if not already present\n",
    "    Return:\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    '''\n",
    "    # Add .pickle filetype if necessary and requested\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "        \n",
    "    # Save file\n",
    "    with open(filename, \"wb\") as outfile:  \n",
    "        pickle.dump(dictionary, outfile)\n",
    "        outfile.close()\n",
    "    \n",
    "    return filename\n",
    "        \n",
    "def load_dict(filename, verbose=False):\n",
    "    '''\n",
    "    Loads dictionary of metrics from given filename\n",
    "    \n",
    "    Params:\n",
    "    - filename (str): file to load\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, \n",
    "        .pickle extension appended to filename if not already present\n",
    "    Return\n",
    "    - dictionary (dict): data found in file\n",
    "    - None (None): return None val in case exception is raised and dictionary file does not exist\n",
    "    '''\n",
    "    # Add .pickle filetype if necessary and requested\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "    \n",
    "    # Load file if exists\n",
    "    try:\n",
    "        with open(filename, 'rb') as pickle_file: \n",
    "            dictionary = pickle.load(pickle_file) \n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict to store {name: dataset}\n",
    "dataset_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer that turns text samples into token vector \n",
    "vectorizer = CountVectorizer(analyzer='char', \n",
    "                             tokenizer=word_tokenize, \n",
    "                             stop_words=stopwords.words('english'))\n",
    "\n",
    "# NOTE: Additional testing was done with a unique vectorizer for each data set where the maximum\n",
    "# number of features considered was ~10% of the overall features (based on other studies implementations)\n",
    "# but performance appeared to be poorer. Since there was amble time for training, decided to utilize\n",
    "# all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yelp data sets\n",
    "yelp_test_df = pd.read_csv('../data/yelp_review_polarity_csv/test.csv', names=['label', 'data']) \n",
    "yelp_train_df = pd.read_csv('../data/yelp_review_polarity_csv/train.csv', names=['label', 'data']) \n",
    "\n",
    "# Since yelp data set is already split into test and train, recombine\n",
    "yelp_df = pd.concat([yelp_test_df, yelp_train_df])\n",
    "\n",
    "# Data set is too large to work with in memory since I don't have 2TiB of RAM just lying around, \n",
    "# so we're cutting the data down into a more workable size\n",
    "yelp_df = yelp_df.sample(n=32000,replace=False,axis='index')\n",
    "\n",
    "# Change 1, 2 label to 0, 1 for uniformity with other data sets\n",
    "# Data set has 1 for negative and 2 for positive, so we switch 0 to negative and 1 to positive\n",
    "yelp_df['label'] = yelp_df['label'].apply(lambda label: 0 if label == 1 else 1)\n",
    "\n",
    "# Transform data into vectorized format\n",
    "yelp_df['data'] = vectorizer.fit_transform(yelp_df['data']).toarray()\n",
    "\n",
    "# Transform df to np array for easier use & add info to dict\n",
    "yelp_data = yelp_df.values\n",
    "dataset_dict['yelp'] = yelp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity/Objectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "subjectivity_df = pd.read_csv('../data/subjectobject/subjectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "objectivity_df = pd.read_csv('../data/subjectobject/objectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "\n",
    "# Add labels (subjective is 0, objective is 1)\n",
    "subjectivity_df['label'] = 0\n",
    "objectivity_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "sub_ob_df = pd.concat([subjectivity_df, objectivity_df])\n",
    "sub_ob_df = sub_ob_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "# Transform data into vectorized format\n",
    "sub_ob_df['data'] = vectorizer.fit_transform(sub_ob_df['data']).toarray()\n",
    "\n",
    "# Transform df to np array, and add to dict\n",
    "sub_ob_data = sub_ob_df.values\n",
    "dataset_dict['sub_ob'] = sub_ob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "clickbait_df = pd.read_csv('../data/clickbait/clickbait_data', sep='\\n', names=['data'])\n",
    "nonclickbait_df = pd.read_csv('../data/clickbait/non_clickbait_data', sep='\\n', names=['data'])\n",
    "\n",
    "# Add labels (clickbait is 0, non-clickbait is 1)\n",
    "nonclickbait_df['label'] = 0\n",
    "clickbait_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "clickbait_df = pd.concat([clickbait_df, nonclickbait_df])\n",
    "clickbait_df = clickbait_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "# Transform data into vectorized format\n",
    "clickbait_df['data'] = vectorizer.fit_transform(clickbait_df['data']).toarray()\n",
    "\n",
    "#Transform df to np array, and add to dict\n",
    "clickbait_data = clickbait_df.values\n",
    "dataset_dict['clickbait'] = clickbait_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(estimator, param_grid, dataset_dict, scoring='accuracy', n_jobs=1, verbose=1, save=True):\n",
    "    '''\n",
    "    Takes data nd model information and returns a dictionary of metrics on the best estimator for each data \n",
    "    set via grid search\n",
    "    \n",
    "    Params:\n",
    "    - estimator: estimator object to use\n",
    "    - param_grid (dict or list of dicts): values to perform grid search over\n",
    "    - dataset_dict (dict): (name: dataset) paired dictionary for all datasets to return best estimator for\n",
    "    - saving='accuracy' (str): specifies how to rank each estimator\n",
    "    - n_jobs=1 (int): number of cores to run training on. -1 includes all cores\n",
    "    - verbose=1 (int): specifies if output messages should be provided\n",
    "    - save (bool): flag for if dictionary should be saved\n",
    "    Returns:\n",
    "    - clf: gridsearch object with best performance\n",
    "    - metric_dict (dict): returns dataset of the form {(name, trial#): {metric_name: metric, model: best_estimator}}\n",
    "    '''\n",
    "    \n",
    "    # Make sure proper data was passed in\n",
    "    assert type(dataset_dict) == dict, 'Please pass in a correct dataset_dict'\n",
    "    assert type(param_grid) in [list, set, tuple, dict], 'Unexpected data type passed in for param_grid'\n",
    "    if type(param_grid) is not dict:\n",
    "        assert type(param_grid[0]) == dict, 'Unexpected data type passed in for param_grid'\n",
    "    \n",
    "    metric_dict = {}\n",
    "    clf = GridSearchCV(estimator=estimator, \n",
    "                       param_grid=param_grid, \n",
    "                       cv=5, n_jobs=n_jobs, \n",
    "                       verbose=verbose, \n",
    "                       scoring=scoring)\n",
    "    \n",
    "    # Analyze every data set (corresponding to whatever data input cells were ran)\n",
    "    for name, dataset in dataset_dict.items():\n",
    "        X, y = dataset[:, 1:], dataset[:, :1] #Treats first column as label\n",
    "        for i in range(3): # Completes 3 trials\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "            clf.fit(X_train, y_train.ravel()) # Fit training data to model\n",
    "\n",
    "            # Gather training set metrics\n",
    "            y_train_pred = clf.predict(X_train)\n",
    "            acc_train = accuracy_score(y_train, y_train_pred)\n",
    "            precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "\n",
    "            # Gather testing set metrics\n",
    "            y_test_pred = clf.predict(X_test) # Predict test values using best parameters from classifier\n",
    "            acc_test = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "            precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "\n",
    "            # Save metrics to dict for further analysis\n",
    "            metric_dict[(name, i)] = {'acc_test': acc_test, \n",
    "                                      'acc_train': acc_train, \n",
    "                                      'precision_test': precision_test, \n",
    "                                      'precision_train': precision_train, \n",
    "                                      'recall_test': recall_test, \n",
    "                                      'recall_train': recall_train,\n",
    "                                      'f1_test': f1_test, \n",
    "                                      'f1_train': f1_train, \n",
    "                                      'model': clf, \n",
    "                                      'cv_results': clf.cv_results_} # Add metrics to dict for analysis\n",
    "            if save:\n",
    "                # Save checkpoint results in case of hardware failure\n",
    "                loc_str = estimator.__class__.__name__ # this just gets clf type (eg SVC, LogisticRegression, etc)\n",
    "                \n",
    "                # Checks if the output path already exists, and makes it if not\n",
    "                if not os.path.isdir('../checkpoints/{}'.format(loc_str)):\n",
    "                    print('Creating {} directory now'.format(loc_str))\n",
    "                    os.mkdir(os.joinpath('..', 'checkpoints', loc_str))\n",
    "                    save_dict(metric_dict, '../checkpoints/{loc_str}/{}_{}_{}.pickle'.format(loc_str, name, i))\n",
    "    \n",
    "    return clf, metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of parameters to search over for SVM\n",
    "c_vals = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "param_grid_svc = [{'kernel': ['linear'], \n",
    "                   'C': c_vals}, \n",
    "                  {'kernel': ['poly'], \n",
    "                   'degree': [2,3], \n",
    "                   'C': c_vals}, \n",
    "                  {'kernel': ['rbf'], \n",
    "                   'gamma': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2], \n",
    "                   'C': c_vals}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier, and then find best parameters via cv grid search\n",
    "svc = SVC()\n",
    "svm_clf, svm_metric_dict = \n",
    "    get_best_model(svc, param_grid_svc, dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create model & grid search object\n",
    "clf_svc = GridSearchCV(estimator=svc, \n",
    "                       param_grid=param_grid_svm, \n",
    "                       cv=5, \n",
    "                       n_jobs=3, \n",
    "                       verbose=10, \n",
    "                       scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates LogisticRegression parameter grid\n",
    "param_grid_logreg = [{'penalty': ['l2'], \n",
    "                      'C': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]}, \n",
    "                     {'penalty': ['none']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier, and then find best parameters via cv grid search\n",
    "logreg = LogisticRegression()\n",
    "logreg_clf, logreg_metric_dict = \n",
    "    get_best_model(logreg, param_grid_logreg, dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf_logreg = GridSearchCV(estimator=logreg, \n",
    "                          param_grid=param_grid_logreg, \n",
    "                          cv=5, \n",
    "                          n_jobs=3, \n",
    "                          verbose=10, \n",
    "                          scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Random Forest parameter grid\n",
    "param_grid_randomforest = {'n_estimators': [128, 256, 512, 1024, 2048, 4096, 8192, 16384], \n",
    "                           'max_features': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier, and then find best parameters via cv grid search\n",
    "randomforest = RandomForestClassifier()\n",
    "randomforest_clf, random_forest_metric_dict = \n",
    "    get_best_model(randomforest, param_grid_randomforest, dataset_dict, n_jobs=2, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf_randomforest = GridSearchCV(estimator=randomforest, \n",
    "                                param_grid=param_grid_randomforest, \n",
    "                                cv=5, \n",
    "                                n_jobs=3, \n",
    "                                verbose=10, \n",
    "                                scoring='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
