{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install statements for all utilized libraries (uncomment which are needed)\n",
    "#!pip3 install pandas # installs numpy with it \n",
    "#!pip3 install numpy\n",
    "#!pip3 install pickle\n",
    "#!pip3 install sklearn\n",
    "#!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Misc\n",
    "import pickle # saving/loading metrics\n",
    "import os\n",
    "\n",
    "# ML\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dictionary, filename, verbose=False):\n",
    "    '''\n",
    "    Saves dictionary object a,s a pickle file for reloading and easy viewing\n",
    "    \n",
    "    Args:\n",
    "    - dictionary (dict): data to be saved\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .json extension appended to filename if not already present\n",
    "    Return:\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    '''\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "        \n",
    "    with open(filename, \"wb\") as outfile:  \n",
    "        pickle.dump(dictionary, outfile)\n",
    "        outfile.close()\n",
    "    \n",
    "    return filename\n",
    "        \n",
    "def load_dict(filename, verbose=False):\n",
    "    '''\n",
    "    Loads dictionary of metrics from given filename\n",
    "    \n",
    "    Args:\n",
    "    - filename (str): file to load\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .pickle extension appended to filename if not already present\n",
    "    Return\n",
    "    - dictionary (dict): data found in file\n",
    "    - None (None): return None val in case exception is raised and dictionary file does not exist\n",
    "    '''\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'rb') as pickle_file: \n",
    "            dictionary = pickle.load(pickle_file) \n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict to store {name: dataset}\n",
    "dataset_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer that turns text samples into token vector \n",
    "vectorizer = CountVectorizer(analyzer='char', tokenizer=word_tokenize, stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yelp data sets\n",
    "yelp_test_df = pd.read_csv('../data/yelp_review_polarity_csv/test.csv', names=['label', 'data']) \n",
    "yelp_train_df = pd.read_csv('../data/yelp_review_polarity_csv/train.csv', names=['label', 'data']) \n",
    "\n",
    "# Since yelp data set is already split into test and train, recombine\n",
    "yelp_df = pd.concat([yelp_test_df, yelp_train_df])\n",
    "\n",
    "# Data set is too large to work with in memory since I don't have 2TiB of RAM just lying around, so we're cutting the data down\n",
    "#yelp_df = yelp_df.sample(n=16000,replace=False,axis='index')\n",
    "\n",
    "# Change 1, 2 label to 0, 1 for uniformity with other data sets\n",
    "# Data set has 1 for negative and 2 for positive, so we switch 0 to negative and 1 to positive\n",
    "yelp_df['label'] = yelp_df['label'].apply(lambda label: 0 if label == 1 else 1)\n",
    "\n",
    "#Vectorize\n",
    "yelp_df['data'] = vectorizer.fit_transform(yelp_df['data']).toarray()\n",
    "\n",
    "# Transform df to np array for easier use & add info to dict\n",
    "yelp_data = yelp_df.values\n",
    "dataset_dict['yelp'] = yelp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity/Objectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "subjectivity_df = pd.read_csv('../data/subjectobject/subjectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "objectivity_df = pd.read_csv('../data/subjectobject/objectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "\n",
    "# Add labels (subjective is 0, objective is 0)\n",
    "subjectivity_df['label'] = 0\n",
    "objectivity_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "sub_ob_df = pd.concat([subjectivity_df, objectivity_df])\n",
    "sub_ob_df = sub_ob_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "#Vectorize\n",
    "sub_ob_df['data'] = vectorizer.fit_transform(sub_ob_df['data']).toarray()\n",
    "\n",
    "#Transform df to np array, and add to dict\n",
    "sub_ob_data = sub_ob_df.values\n",
    "dataset_dict['sub_ob'] = sub_ob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "clickbait_df = pd.read_csv('../data/clickbait/clickbait_data', sep='\\n', names=['data'])\n",
    "nonclickbait_df = pd.read_csv('../data/clickbait/non_clickbait_data', sep='\\n', names=['data'])\n",
    "\n",
    "# Add labels (subjective is 0, objective is 0)\n",
    "nonclickbait_df['label'] = 0\n",
    "clickbait_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "clickbait_df = pd.concat([clickbait_df, nonclickbait_df])\n",
    "clickbait_df = clickbait_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "#Vectorize\n",
    "clickbait_df['data'] = vectorizer.fit_transform(clickbait_df['data']).toarray()\n",
    "\n",
    "#Transform df to np array, and add to dict\n",
    "clickbait_data = clickbait_df.values\n",
    "dataset_dict['clickbait'] = clickbait_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(estimator, param_grid, dataset_dict, scoring='accuracy', n_jobs=1, verbose=1, save=True):\n",
    "    '''\n",
    "    Takes data nd model information and returns a dictionary of metrics on the best estimator for aech data set via grid search\n",
    "    \n",
    "    Parameters:\n",
    "    - estimator: estimator object to use\n",
    "    - param_grid (dict or list of dicts): values to perform grid search over\n",
    "    - dataset_dict (dict): (name: dataset) paired dictionary for all datasets to return best estimator for\n",
    "    - saving='accuracy' (str): specifies how to rank each estimator\n",
    "    - n_jobs=1 (int): number of cores to run training on. -1 includes all cores\n",
    "    - verbose=1 (int): specifies if output messages should be provided\n",
    "    - save (bool): flag for if dictionary should be saved\n",
    "    Returns:\n",
    "    - clf: gridsearch object with best performance\n",
    "    - metric_dict (dict): returns dataset of the form {(name, trial#): {metric_name: metric, model: best_estimator}}\n",
    "    '''\n",
    "    \n",
    "    # Make sure proper data was passed in\n",
    "    assert type(dataset_dict) == dict, 'Please pass in a correct dataset_dict'\n",
    "    assert type(param_grid) in [list, set, tuple, dict], 'Unexpected data type passed in for param_grid'\n",
    "    if type(param_grid) is not dict:\n",
    "        assert type(param_grid[0]) == dict, 'Unexpected data type passed in for param_grid'\n",
    "    \n",
    "    metric_dict = {}\n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "    \n",
    "    for name, dataset in dataset_dict.items():\n",
    "        X, y = dataset[:, 1:], dataset[:, :1] #Treats first column as label\n",
    "        for i in range(3):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "            clf.fit(X_train, y_train.ravel()) # Fit training data to model\n",
    "\n",
    "            y_train_pred = clf.predict(X_train)\n",
    "            acc_train = accuracy_score(y_train, y_train_pred)\n",
    "            precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "\n",
    "            y_test_pred = clf.predict(X_test) # Predict test values using best parameters from classifier\n",
    "            acc_test = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "            precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "\n",
    "            metric_dict[(name, i)] = {'acc_test': acc_test, 'acc_train': acc_train, 'precision_test': precision_test, 'precision_train': precision_train, 'recall_test': recall_test, 'recall_train': recall_train,\n",
    "                                          'f1_test': f1_test, 'f1_train': f1_train, 'model': clf, 'cv_results': clf.cv_results_} # Add metrics to dict for analysis\n",
    "            if save:\n",
    "                # Save checkpoint results in case of hardware failure\n",
    "                loc_str = estimator.__class__.__name__\n",
    "                if not os.path.isdir('../checkpoints/{}'.format(loc_str)):\n",
    "                    print('Creating {} directory now'.format(loc_str))\n",
    "                    os.mkdir(os.joinpath('..', 'checkpoints', loc_str))\n",
    "                    save_dict(metric_dict, '../checkpoints/{loc_str}/{}_{}_{}.pickle'.format(loc_str, name, i))\n",
    "    \n",
    "    return clf, metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of parameters to search over for SVM\n",
    "c_vals = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "param_grid_svc = [{'kernel': ['linear'], 'C': c_vals}, {'kernel': ['poly'], 'degree': [2,3], 'C': c_vals}, {'kernel': ['rbf'], 'gamma': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2], 'C': c_vals}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svm_clf, svm_metric_dict = get_best_model(svc, param_grid_svc, dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create model & grid search object\n",
    "clf_svc = GridSearchCV(estimator=svc, param_grid=param_grid_svm, cv=5, n_jobs=3, verbose=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for name, dataset in dataset_dict.items():\n",
    "    # Get data\n",
    "    X, y = dataset[:, 1:], dataset[:, :1] #Treats first column as label\n",
    "    for i in range(3):\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "        \n",
    "        clf_svc.fit(X_train, y_train.ravel()) # Fit training data to model\n",
    "        \n",
    "        # Train set performance\n",
    "        y_train_pred = clf_svc.predict(X_train)\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "        \n",
    "        # Test set performance\n",
    "        y_test_pred = clf_svc.predict(X_test) # Predict test values using best parameters from classifier\n",
    "        acc_test = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "        precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "        \n",
    "        svm_metric_dict[(name, i)] = {'acc_test': acc_test, 'acc_train': acc_train, 'precision_test': precision_test, 'precision_train': precision_train, 'recall_test': recall_test, 'recall_train': recall_train,\n",
    "                                      'f1_test': f1_test, 'f1_train': f1_train, 'model': clf_svc, 'cv_results': clf_svc.cv_results_} # Add metrics to dict for analysis\n",
    "        save_dict(svm_metric_dict, '../checkpoints/svm/svm_{}_{}.pickle'.format(name, i)) # Save checkpoint results in case of hardware failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_logreg = [{'penalty': ['l2'], 'C': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]}, {'penalty': ['none']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg_clf, logreg_metric_dict = get_best_model(logreg, param_grid_logreg, dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf_logreg = GridSearchCV(estimator=logreg, param_grid=param_grid_logreg, cv=5, n_jobs=3, verbose=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for name, dataset in dataset_dict.items():\n",
    "    X, y = dataset[:, 1:], dataset[:, :1]\n",
    "    for i in range(3):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "        \n",
    "        clf_logreg.fit(X_train, y_train.ravel())\n",
    "        \n",
    "        # Train set performance\n",
    "        y_train_pred = clf_logreg.predict(X_train)\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "        \n",
    "        # Test set performance\n",
    "        y_test_pred = clf_logreg.predict(X_test) # Predict test values using best parameters from classifier\n",
    "        acc_test = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "        precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "        \n",
    "        logreg_metric_dict[(name, i)] = {'acc_test': acc_test, 'acc_train': acc_train, 'precision_test': precision_test, 'precision_train': precision_train, 'recall_test': recall_test, 'recall_train': recall_train,\n",
    "                                      'f1_test': f1_test, 'f1_train': f1_train, 'model': clf_logreg, 'cv_results': clf_logreg.cv_results_} # Add metrics to dict for analysis\n",
    "        #save_dict(logreg_metric_dict, '../checkpoints/logreg/logreg_{}_{}.pickle'.format(name, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_randomforest = {'n_estimators': [1024], 'max_features': [1,2,4,6,8,12,16,20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier()\n",
    "randomforest_clf, random_forest_metric_dict = get_best_model(randomforest, param_grid_randomforest, dataset_dict, n_jobs=3, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf_randomforest = GridSearchCV(estimator=randomforest, param_grid=param_grid_randomforest, cv=5, n_jobs=3, verbose=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for name, dataset in dataset_dict.items():\n",
    "    X, y = dataset[:, 1:], dataset[:, :1]\n",
    "    for i in range(3):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "        \n",
    "        clf_randomforest.fit(X_train, y_train.ravel())\n",
    "        \n",
    "        # Train set performance\n",
    "        y_train_pred = clf_randomforest.predict(X_train)\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "        \n",
    "        # Test set performance\n",
    "        y_test_pred = clf_randomforest.predict(X_test) # Predict test values using best parameters from classifier\n",
    "        acc_test = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "        precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "        \n",
    "        randomforest_metric_dict[(name, i)] = {'acc_test': acc_test, 'acc_train': acc_train, 'precision_test': precision_test, 'precision_train': precision_train, 'recall_test': recall_test, 'recall_train': recall_train,\n",
    "                                      'f1_test': f1_test, 'f1_train': f1_train, 'model': clf_randomforest, 'cv_results': clf_randomforest.cv_results_} # Add metrics to dict for analysis\n",
    "        #save_dict(randomforest_metric_dict, '../checkpoints/randomforest/randomforest_{}_{}.pickle'.format(name, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
