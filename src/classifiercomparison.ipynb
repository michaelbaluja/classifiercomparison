{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install statements for all utilized libraries (uncomment which are needed)\n",
    "#!pip3 install pandas # installs numpy with it \n",
    "#!pip3 install numpy\n",
    "#!pip3 install pickle\n",
    "#!pip3 install sklearn\n",
    "#!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Misc\n",
    "import pickle # saving/loading metrics\n",
    "\n",
    "# ML\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dictionary, filename, verbose=False):\n",
    "    '''\n",
    "    Saves dictionary object as a pickle file for reloading and easy viewing\n",
    "    \n",
    "    Args:\n",
    "    - dictionary (dict): data to be saved\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .json extension appended to filename if not already present\n",
    "    Return:\n",
    "    - filename (str): filename for dictionary to be stored in\n",
    "    '''\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "        \n",
    "    with open(filename, \"wb\") as outfile:  \n",
    "        pickle.dump(dictionary, outfile)\n",
    "        outfile.close()\n",
    "    \n",
    "    return filename\n",
    "        \n",
    "def load_dict(filename, verbose=False):\n",
    "    '''\n",
    "    Loads dictionary of metrics from given filename\n",
    "    \n",
    "    Args:\n",
    "    - filename (str): file to load\n",
    "    - verbose=False (bool): sepcifies if exact filename should be used. if False, .pickle extension appended to filename if not already present\n",
    "    Return\n",
    "    - dictionary (dict): data found in file\n",
    "    - None (None): return None val in case exception is raised and dictionary file does not exist\n",
    "    '''\n",
    "    if (not verbose) and ('.pickle' not in filename):\n",
    "        filename += '.pickle'\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'rb') as pickle_file: \n",
    "            dictionary = pickle.load(pickle_file) \n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict to store {name: dataset}\n",
    "dataset_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer that turns text samples into token vector \n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# Create vectorizer that turns text samples into token vector \n",
    "yelp_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, stop_words=stopwords.words('english'), max_features=6000)\n",
    "\n",
    "# Load yelp data sets\n",
    "yelp_test_df = pd.read_csv('../data/yelp_review_polarity_csv/test.csv', names=['label', 'data']) \n",
    "yelp_train_df = pd.read_csv('../data/yelp_review_polarity_csv/train.csv', names=['label', 'data']) \n",
    "\n",
    "# Since yelp data set is already split into test and train, recombine\n",
    "yelp_df = pd.concat([yelp_test_df, yelp_train_df])\n",
    "\n",
    "# Data set is too large to work with in memory since I don't have 2TiB of RAM just lying around, so we're cutting the data down\n",
    "yelp_df = yelp_df.sample(n=16000,replace=False,axis='index')\n",
    "\n",
    "# Change 1, 2 label to 0, 1 for uniformity with other data sets\n",
    "# Data set has 1 for negative and 2 for positive, so we switch 0 to negative and 1 to positive\n",
    "yelp_df['label'] = yelp_df['label'].apply(lambda label: 0 if label == 1 else 1)\n",
    "\n",
    "#Vectorize\n",
    "yelp_df['data'] = vectorizer.fit_transform(yelp_df['data']).toarray()\n",
    "\n",
    "# Transform df to np array for easier use & add info to dict\n",
    "yelp_data = yelp_df.values\n",
    "dataset_dict['yelp'] = yelp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity/Objectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer that turns text samples into token vector \n",
    "subob_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, stop_words=stopwords.words('english'), max_features=2200)\n",
    "\n",
    "# Load data sets\n",
    "subjectivity_df = pd.read_csv('../data/subjectobject/subjectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "objectivity_df = pd.read_csv('../data/subjectobject/objectivity.txt', sep='\\n', encoding='latin-1', names=['data'])\n",
    "\n",
    "# Add labels (subjective is 0, objective is 0)\n",
    "subjectivity_df['label'] = 0\n",
    "objectivity_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "sub_ob_df = pd.concat([subjectivity_df, objectivity_df])\n",
    "sub_ob_df = sub_ob_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "#Vectorize\n",
    "sub_ob_df['data'] = vectorizer.fit_transform(sub_ob_df['data']).toarray()\n",
    "\n",
    "#Transform df to np array, and add to dict\n",
    "sub_ob_data = sub_ob_df.values\n",
    "dataset_dict['sub_ob'] = sub_ob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# Create vectorizer that turns text samples into token vector \n",
    "clickbait_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, stop_words=stopwords.words('english'), max_features=2500)\n",
    "\n",
    "# Load data sets\n",
    "clickbait_df = pd.read_csv('../data/clickbait/clickbait_data', sep='\\n', names=['data'])\n",
    "nonclickbait_df = pd.read_csv('../data/clickbait/non_clickbait_data', sep='\\n', names=['data'])\n",
    "\n",
    "# Add labels (subjective is 0, objective is 0)\n",
    "nonclickbait_df['label'] = 0\n",
    "clickbait_df['label'] = 1\n",
    "\n",
    "# Combine data sets and rearrange columns for uniformity\n",
    "clickbait_df = pd.concat([clickbait_df, nonclickbait_df])\n",
    "clickbait_df = clickbait_df.reindex(columns=['label', 'data'])\n",
    "\n",
    "#Vectorize\n",
    "clickbait_df['data'] = vectorizer.fit_transform(clickbait_df['data']).toarray()\n",
    "\n",
    "#Transform df to np array, and add to dict\n",
    "clickbait_data = clickbait_df.values\n",
    "dataset_dict['clickbait'] = clickbait_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metric dict\n",
    "svm_metric_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of parameters to search over for SVM\n",
    "c_vals = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "param_grid_svm = [{'kernel': ['linear'], 'C': c_vals}, {'kernel': ['poly'], 'degree': [0,2,3], 'C': c_vals}, {'kernel': ['rbf'], 'gamma': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2], 'C': c_vals}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model & grid search object\n",
    "svc = SVC()\n",
    "clf_svc = GridSearchCV(estimator=svc, param_grid=param_grid_svm, cv=5, n_jobs=3, verbose=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 143 candidates, totalling 715 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=3)]: Done 307 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=3)]: Done 386 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=3)]: Done 415 tasks      | elapsed:   43.4s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=3)]: Done 475 tasks      | elapsed:   50.4s\n",
      "[Parallel(n_jobs=3)]: Done 506 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=3)]: Done 539 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=3)]: Done 572 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=3)]: Done 607 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 679 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 715 out of 715 | elapsed:  1.3min finished\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 143 candidates, totalling 715 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=3)]: Done 307 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=3)]: Done 386 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=3)]: Done 415 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=3)]: Done 475 tasks      | elapsed:   50.5s\n",
      "[Parallel(n_jobs=3)]: Done 506 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=3)]: Done 539 tasks      | elapsed:   58.2s\n",
      "[Parallel(n_jobs=3)]: Done 572 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=3)]: Done 607 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 679 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 715 out of 715 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 143 candidates, totalling 715 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=3)]: Done  79 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=3)]: Done  92 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=3)]: Done 107 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=3)]: Done 122 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=3)]: Done 139 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=3)]: Done 156 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=3)]: Done 175 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=3)]: Done 215 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=3)]: Done 236 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=3)]: Done 259 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=3)]: Done 282 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=3)]: Done 307 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=3)]: Done 359 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=3)]: Done 386 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=3)]: Done 415 tasks      | elapsed:   43.4s\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in dataset_dict.items():\n",
    "    # Get data\n",
    "    X, y = dataset[:, 1:], dataset[:, :1] #Treats first column as label\n",
    "    for i in range(3):\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "        \n",
    "        clf_svc.fit(X_train, y_train.ravel()) # Fit training data to model\n",
    "        \n",
    "        # Train set performance\n",
    "        y_train_pred = clf_svc.predict(X_train)\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "        \n",
    "        # Test set performance\n",
    "        y_test_pred = clf_svc.predict(X_test) # Predict test values using best parameters from classifier\n",
    "        acc_test = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "        precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "        \n",
    "        svm_metric_dict[(name, i)] = {'acc_test': acc_test, 'acc_train': acc_train, 'precision_test': precision_test, 'precision_train': precision_train, 'recall_test': recall_test, 'recall_train': recall_train,\n",
    "                                      'f1_test': f1_test, 'f1_test': f1_train, 'model_test': clf_svc, 'cv_results': clf_svc.cv_results_} # Add metrics to dict for analysis\n",
    "        save_dict(svm_metric_dict, '../checkpoints/svm/svm_{}_{}.pickle'.format(name, i)) # Save checkpoint results in case of hardware failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "###### DEPRACATED FOR MULTITHREAD SKLEARN GRID SEARCH, KEPT IN CASE OF MEASURING OTHER METRICS\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cycle across each param combo\n",
    "performance_dict = {}\n",
    "for param_dict in tqdm(list(ParameterGrid(param_grid))):\n",
    "    performance = 0\n",
    "    C, degree, gamma, kernel = param_dict.values()\n",
    "    if ((kernel in ('linear', 'rbf') and degree > 0) or  # Don't want to run linear or rbf with polynomial degrees (degree will be ignored but we'll get duplicate trials)\n",
    "        (kernel == 'poly' and degree == 0) or # Don't want polynomial with degree 0\n",
    "        (kernel in ('linear', 'poly') and gamma > 0) or # Don't want linear or poly with gamma param\n",
    "        (kernel == 'rbf' and gamma == 0)): # Don't want rbf with 0 gamma\n",
    "        continue\n",
    "    # Do k fold validation\n",
    "    for train, validate in kf.split(X_letter_train):\n",
    "        X_letter_train_cross, X_letter_val_cross, y_letter_train_cross, y_letter_val_cross = X_letter_train[train], X_letter_train[validate], y_letter_train[train], y_letter_train[validate] # get data folds\n",
    "        svm_letter = SVC(C=C, degree=degree, kernel=kernel) # create the model #NOTE: not scaling because all data appears to follow the same scaling regardless\n",
    "        svm_letter.fit(X_letter_train_cross, y_letter_train_cross.ravel()) # fit the model\n",
    "        y_letter_val_cross_pred = svm_letter.predict(X_letter_val_cross) # predict validation data\n",
    "        performance += accuracy_score(y_letter_val_cross, y_letter_val_cross_pred) # keep track of performance\n",
    "    # Average the performance\n",
    "    performance /= 5\n",
    "    \n",
    "    # Add performance info to dict\n",
    "    performance_dict[(C, degree, kernel)] = performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_metric_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_logreg = [{'penalty': ['l2'], 'C': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]}, {'penalty': ['none']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "clf_logreg = GridSearchCV(estimator=logreg, param_grid=param_grid_logreg, cv=5, n_jobs=3, verbose=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0186s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0565s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0299s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0555s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0114s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0190s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0318s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0516s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0141s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0186s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0384s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0534s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0091s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0153s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0279s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0548s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0107s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0184s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0227s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0634s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0142s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0148s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0222s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0541s.) Setting batch_size=16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0075s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0190s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0206s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0630s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.1s finished\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0129s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0123s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0199s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0676s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0104s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0165s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=3)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0277s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=3)]: Done  50 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Batch computation too fast (0.0897s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=3)]: Done  70 out of  70 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in dataset_dict.items():\n",
    "    X, y = dataset[:, 1:], dataset[:, :1]\n",
    "    for i in range(3):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "        \n",
    "        clf_logreg.fit(X_train, y_train.ravel())\n",
    "        \n",
    "        # Train set performance\n",
    "        y_train_pred = clf_logreg.predict(X_train)\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred)\n",
    "        \n",
    "        # Test set performance\n",
    "        y_test_pred = clf_logreg.predict(X_test) # Predict test values using best parameters from classifier\n",
    "        acc_test = accuracy_score(y_test, y_test_pred) # Get accuracy for predictions\n",
    "        precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred)\n",
    "        \n",
    "        logreg_metric_dict[(name, i)] = {'acc_test': acc_test, 'acc_train': acc_train, 'precision_test': precision_test, 'precision_train': precision_train, 'recall_test': recall_test, 'recall_train': recall_train,\n",
    "                                      'f1_test': f1_test, 'f1_train': f1_train, 'model': clf_logreg, 'cv_results': clf_logreg.cv_results_} # Add metrics to dict for analysis\n",
    "        save_dict(logreg_metric_dict, '../checkpoints/logreg/logreg_{}_{}.pickle'.format(name, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = load_dict('../checkpoints/svm/svm_clickbait_0.pickle')\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
